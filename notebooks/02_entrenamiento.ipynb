{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuaderno de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola Mundo\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Cargar datos\n",
    "print(\"=== Cargando datos ===\")\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "\n",
    "df_proc = pd.read_csv(\"/content/drive/MyDrive/DeepLearning/House_Rent_Dataset.csv\")\n",
    "\n",
    "# Filtrar registros con categorías poco frecuentes\n",
    "df_proc = df_proc[(df_proc['Area Type'] != 'Built Area') &\n",
    "                  (df_proc['Point of Contact'] != 'Contact Builder')]\n",
    "\n",
    "print(f\"Dimensiones del dataset después de filtrado: {df_proc.shape}\")\n",
    "\n",
    "# Extraer información de Floor\n",
    "def extract_floor_info(floor_str):\n",
    "    \"\"\"\n",
    "    Extrae información del piso y total de pisos.\n",
    "    Retorna: (número_de_piso, total_de_pisos)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        floor_str = str(floor_str).strip()\n",
    "\n",
    "        # Procesar valores especiales\n",
    "        if \"Upper Basement\" in floor_str:\n",
    "            floor_num = -1\n",
    "        elif \"Lower Basement\" in floor_str:\n",
    "            floor_num = -2\n",
    "        elif \"Ground\" in floor_str:\n",
    "            floor_num = 0\n",
    "        else:\n",
    "            # Extraer número de piso\n",
    "            parts = floor_str.split(\"out of\")\n",
    "            if len(parts) > 0 and parts[0].strip().isdigit():\n",
    "                floor_num = int(parts[0].strip())\n",
    "            else:\n",
    "                floor_num = None\n",
    "\n",
    "        # Extraer total de pisos\n",
    "        if \"out of\" in floor_str:\n",
    "            parts = floor_str.split(\"out of\")\n",
    "            if len(parts) > 1 and parts[1].strip().isdigit():\n",
    "                total_floors = int(parts[1].strip())\n",
    "            else:\n",
    "                total_floors = None\n",
    "        else:\n",
    "            total_floors = None\n",
    "\n",
    "        return floor_num, total_floors\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "# Aplicar extracción\n",
    "print(\"=== Procesando columna Floor ===\")\n",
    "df_proc[['Floor_Number', 'Total_Floors']] = df_proc['Floor'].apply(\n",
    "    lambda x: pd.Series(extract_floor_info(x))\n",
    ")\n",
    "\n",
    "# Calcular ratio (con manejo de casos nulos)\n",
    "df_proc['Floor_Ratio'] = None\n",
    "mask = (~df_proc['Floor_Number'].isna()) & (~df_proc['Total_Floors'].isna()) & (df_proc['Total_Floors'] > 0)\n",
    "df_proc.loc[mask, 'Floor_Ratio'] = df_proc.loc[mask, 'Floor_Number'] / df_proc.loc[mask, 'Total_Floors']\n",
    "\n",
    "# Rellenar valores nulos con la mediana\n",
    "df_proc['Floor_Number'] = df_proc['Floor_Number'].fillna(df_proc['Floor_Number'].median())\n",
    "df_proc['Total_Floors'] = df_proc['Total_Floors'].fillna(df_proc['Total_Floors'].median())\n",
    "df_proc['Floor_Ratio'] = df_proc['Floor_Ratio'].fillna(df_proc['Floor_Ratio'].median())\n",
    "\n",
    "# Aplicar transformación logarítmica\n",
    "print(\"=== Aplicando transformación logarítmica ===\")\n",
    "df_proc['Rent_log'] = np.log(df_proc['Rent'])\n",
    "df_proc['Size_log'] = np.log(df_proc['Size'])\n",
    "\n",
    "# Eliminar columnas que no aportan valor predictivo\n",
    "columnas_a_eliminar = ['Posted On', 'Area Locality', 'Floor']\n",
    "X = df_proc.drop(columns=columnas_a_eliminar + ['Rent', 'Rent_log']).copy()\n",
    "y = df_proc['Rent_log'].copy()\n",
    "\n",
    "# Guardar los valores originales para evaluación\n",
    "y_original = df_proc['Rent'].copy()\n",
    "\n",
    "print(\"Columnas después de eliminaciones:\")\n",
    "print(X.columns.tolist())\n",
    "\n",
    "# Verificar valores nulos restantes\n",
    "print(\"\\nValores nulos por columna en X:\")\n",
    "print(X.isna().sum())\n",
    "\n",
    "# Rellenar valores nulos restantes\n",
    "for col in X.columns:\n",
    "    if X[col].isna().any():\n",
    "        if X[col].dtype.kind in 'ifc':  # Numérico\n",
    "            X[col] = X[col].fillna(X[col].median())\n",
    "        else:  # Categórico\n",
    "            X[col] = X[col].fillna(X[col].mode()[0])\n",
    "\n",
    "# Verificar que no quedan valores nulos\n",
    "print(\"\\nValores nulos después de imputación:\")\n",
    "print(X.isna().sum().sum())\n",
    "\n",
    "# Identificar tipos de columnas\n",
    "cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"\\nColumnas categóricas ({len(cat_cols)}):\")\n",
    "print(cat_cols)\n",
    "print(f\"\\nColumnas numéricas ({len(num_cols)}):\")\n",
    "print(num_cols)\n",
    "\n",
    "# División en train/test (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test, y_train_original, y_test_original = train_test_split(\n",
    "    X, y, y_original, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Crear preprocesador (con sparse_output=False para OneHotEncoder)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Aplicar preprocesamiento\n",
    "preprocessor.fit(X_train)\n",
    "X_train_prep = preprocessor.transform(X_train)\n",
    "X_test_prep = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"\\nDatos preprocesados:\")\n",
    "print(f\"X_train_prep: {X_train_prep.shape}\")\n",
    "print(f\"X_test_prep: {X_test_prep.shape}\")\n",
    "\n",
    "# Verificar valores NaN en los datos preprocesados\n",
    "print(f\"NaN en X_train_prep: {np.isnan(X_train_prep).any()}\")\n",
    "print(f\"NaN en X_test_prep: {np.isnan(X_test_prep).any()}\")\n",
    "\n",
    "# Reemplazar NaN en datos preprocesados si hay alguno\n",
    "if np.isnan(X_train_prep).any():\n",
    "    print(\"Reemplazando NaN en datos preprocesados...\")\n",
    "    X_train_prep = np.nan_to_num(X_train_prep)\n",
    "    X_test_prep = np.nan_to_num(X_test_prep)\n",
    "\n",
    "# Definir función para crear modelos\n",
    "def crear_modelo(config):\n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal con la configuración especificada\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # Primera capa (entrada)\n",
    "    model.add(Dense(\n",
    "        config['hidden_layers'][0],\n",
    "        activation=config['activation'],\n",
    "        input_shape=(X_train_prep.shape[1],),\n",
    "        kernel_regularizer=None if config.get('l2_reg') is None else tf.keras.regularizers.l2(config.get('l2_reg'))\n",
    "    ))\n",
    "\n",
    "    if config.get('batch_norm', False):\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    if config.get('dropout', 0) > 0:\n",
    "        model.add(Dropout(config['dropout']))\n",
    "\n",
    "    # Capas ocultas\n",
    "    for units in config['hidden_layers'][1:]:\n",
    "        model.add(Dense(\n",
    "            units,\n",
    "            activation=config['activation'],\n",
    "            kernel_regularizer=None if config.get('l2_reg') is None else tf.keras.regularizers.l2(config.get('l2_reg'))\n",
    "        ))\n",
    "\n",
    "        if config.get('batch_norm', False):\n",
    "            model.add(BatchNormalization())\n",
    "\n",
    "        if config.get('dropout', 0) > 0:\n",
    "            model.add(Dropout(config['dropout']))\n",
    "\n",
    "    # Capa de salida\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compilar modelo\n",
    "    if config.get('optimizer', 'adam') == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=config.get('learning_rate', 0.001))\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=config.get('learning_rate', 0.001))\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Definir experimentos\n",
    "print(\"\\n=== Definiendo experimentos ===\")\n",
    "experimentos = [\n",
    "    {\n",
    "        'name': \"Dropout\",\n",
    "        'hidden_layers': [128, 64, 32],\n",
    "        'activation': 'relu',\n",
    "        'dropout': 0.2,\n",
    "        'learning_rate': 0.001,\n",
    "        'batch_size': 32,\n",
    "        'epochs': 200\n",
    "    },\n",
    "    {\n",
    "        'name': \"L2\",\n",
    "        'hidden_layers': [128, 64, 32],\n",
    "        'activation': 'relu',\n",
    "        'l2_reg': 0.01,\n",
    "        'learning_rate': 0.001,\n",
    "        'batch_size': 32,\n",
    "        'epochs': 200\n",
    "    },\n",
    "    {\n",
    "        'name': \"Combined\",\n",
    "        'hidden_layers': [128, 64, 32],\n",
    "        'activation': 'relu',\n",
    "        'dropout': 0.2,\n",
    "        'l2_reg': 0.01,\n",
    "        'batch_norm': True,\n",
    "        'learning_rate': 0.001,\n",
    "        'batch_size': 32,\n",
    "        'epochs': 200\n",
    "    }\n",
    "]\n",
    "\n",
    "# Función para evaluar modelos (simplificada con transformación logarítmica)\n",
    "def evaluar_modelo(modelo, X_train, y_train, X_test, y_test, y_train_original, y_test_original):\n",
    "    \"\"\"\n",
    "    Evalúa un modelo y devuelve métricas en escalas transformada y original\n",
    "    \"\"\"\n",
    "    # Predicciones en escala logarítmica\n",
    "    y_train_pred_log = modelo.predict(X_train).flatten()\n",
    "    y_test_pred_log = modelo.predict(X_test).flatten()\n",
    "\n",
    "    # Métricas en escala logarítmica\n",
    "    train_mse_log = mean_squared_error(y_train, y_train_pred_log)\n",
    "    test_mse_log = mean_squared_error(y_test, y_test_pred_log)\n",
    "    train_mae_log = mean_absolute_error(y_train, y_train_pred_log)\n",
    "    test_mae_log = mean_absolute_error(y_test, y_test_pred_log)\n",
    "    train_r2_log = r2_score(y_train, y_train_pred_log)\n",
    "    test_r2_log = r2_score(y_test, y_test_pred_log)\n",
    "\n",
    "    # Transformar a escala original\n",
    "    y_train_pred_original = np.exp(y_train_pred_log)\n",
    "    y_test_pred_original = np.exp(y_test_pred_log)\n",
    "\n",
    "    # Métricas en escala original\n",
    "    train_mse = mean_squared_error(y_train_original, y_train_pred_original)\n",
    "    test_mse = mean_squared_error(y_test_original, y_test_pred_original)\n",
    "    train_mae = mean_absolute_error(y_train_original, y_train_pred_original)\n",
    "    test_mae = mean_absolute_error(y_test_original, y_test_pred_original)\n",
    "    train_r2 = r2_score(y_train_original, y_train_pred_original)\n",
    "    test_r2 = r2_score(y_test_original, y_test_pred_original)\n",
    "\n",
    "    # MAPE (error porcentual medio absoluto)\n",
    "    train_mape = np.mean(np.abs((y_train_original - y_train_pred_original) / y_train_original)) * 100\n",
    "    test_mape = np.mean(np.abs((y_test_original - y_test_pred_original) / y_test_original)) * 100\n",
    "\n",
    "    # Métricas resumidas\n",
    "    metricas = {\n",
    "        'train_mse_log': train_mse_log,\n",
    "        'test_mse_log': test_mse_log,\n",
    "        'train_mae_log': train_mae_log,\n",
    "        'test_mae_log': test_mae_log,\n",
    "        'train_r2_log': train_r2_log,\n",
    "        'test_r2_log': test_r2_log,\n",
    "        'train_mse': train_mse,\n",
    "        'test_mse': test_mse,\n",
    "        'train_mae': train_mae,\n",
    "        'test_mae': test_mae,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'train_mape': train_mape,\n",
    "        'test_mape': test_mape\n",
    "    }\n",
    "\n",
    "    return metricas\n",
    "\n",
    "# Entrenar y evaluar cada modelo\n",
    "resultados = []\n",
    "tiempo_inicio_total = time.time()\n",
    "\n",
    "for config in experimentos:\n",
    "    print(f\"\\n=== Entrenando modelo: {config['name']} ===\")\n",
    "    tiempo_inicio = time.time()\n",
    "\n",
    "    # Crear modelo\n",
    "    modelo = crear_modelo(config)\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=30,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=0.0001,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Entrenar modelo\n",
    "    history = modelo.fit(\n",
    "        X_train_prep, y_train,\n",
    "        validation_split=0.2,  # Usar 20% de train como validación\n",
    "        epochs=config['epochs'],\n",
    "        batch_size=config['batch_size'],\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluar modelo\n",
    "    metricas = evaluar_modelo(modelo, X_train_prep, y_train, X_test_prep, y_test,\n",
    "                              y_train_original, y_test_original)\n",
    "\n",
    "    tiempo_fin = time.time()\n",
    "    tiempo_entrenamiento = tiempo_fin - tiempo_inicio\n",
    "\n",
    "    # Guardar resultados\n",
    "    resultado = {\n",
    "        'config': config,\n",
    "        'modelo': modelo,\n",
    "        'history': history.history,\n",
    "        'metricas': metricas,\n",
    "        'tiempo_entrenamiento': tiempo_entrenamiento\n",
    "    }\n",
    "    resultados.append(resultado)\n",
    "\n",
    "    # Mostrar métricas principales\n",
    "    print(f\"Tiempo de entrenamiento: {tiempo_entrenamiento:.2f} segundos\")\n",
    "    print(f\"MSE (test): {metricas['test_mse']:.2f}\")\n",
    "    print(f\"MAE (test): {metricas['test_mae']:.2f}\")\n",
    "    print(f\"MAPE (test): {metricas['test_mape']:.2f}%\")\n",
    "    print(f\"R² (test): {metricas['test_r2']:.4f}\")\n",
    "\n",
    "tiempo_fin_total = time.time()\n",
    "tiempo_total = tiempo_fin_total - tiempo_inicio_total\n",
    "print(f\"\\nTiempo total de experimentación: {tiempo_total/60:.2f} minutos\")\n",
    "\n",
    "# Crear DataFrame con resumen de resultados\n",
    "df_resultados = pd.DataFrame([\n",
    "    {\n",
    "        'Modelo': res['config']['name'],\n",
    "        'Capas': str(res['config']['hidden_layers']),\n",
    "        'Activación': res['config']['activation'],\n",
    "        'Dropout': res['config'].get('dropout', 0),\n",
    "        'L2': res['config'].get('l2_reg', 0),\n",
    "        'BatchNorm': res['config'].get('batch_norm', False),\n",
    "        'MSE': res['metricas']['test_mse'],\n",
    "        'MAE': res['metricas']['test_mae'],\n",
    "        'MAPE (%)': res['metricas']['test_mape'],\n",
    "        'R²': res['metricas']['test_r2'],\n",
    "        'Tiempo (s)': res['tiempo_entrenamiento']\n",
    "    } for res in resultados\n",
    "])\n",
    "\n",
    "# Mostrar resultados ordenados por R²\n",
    "print(\"\\n=== Resumen de resultados ===\")\n",
    "display(df_resultados.sort_values('R²', ascending=False))\n",
    "\n",
    "# Visualizar comparación de pérdida de validación\n",
    "plt.figure(figsize=(12, 6))\n",
    "for res in resultados:\n",
    "    plt.plot(res['history']['val_loss'], label=res['config']['name'])\n",
    "plt.title('Comparación de Pérdida de Validación')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Obtener mejor modelo\n",
    "mejor_idx = df_resultados['R²'].idxmax()\n",
    "mejor_modelo = resultados[mejor_idx]['modelo']\n",
    "mejor_config = resultados[mejor_idx]['config']\n",
    "\n",
    "print(f\"\\n=== Mejor modelo: {mejor_config['name']} ===\")\n",
    "print(f\"R² (test): {df_resultados.iloc[mejor_idx]['R²']:.4f}\")\n",
    "print(f\"MAPE (test): {df_resultados.iloc[mejor_idx]['MAPE (%)']:.2f}%\")\n",
    "\n",
    "# Visualizar predicciones vs valores reales para el mejor modelo\n",
    "y_pred_log = mejor_modelo.predict(X_test_prep).flatten()\n",
    "y_pred_original = np.exp(y_pred_log)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_original, y_pred_original, alpha=0.5)\n",
    "plt.plot([y_test_original.min(), y_test_original.max()],\n",
    "         [y_test_original.min(), y_test_original.max()], 'r--')\n",
    "plt.title('Predicciones vs Valores Reales')\n",
    "plt.xlabel('Precio Real')\n",
    "plt.ylabel('Precio Predicho')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Histograma de errores\n",
    "errores = y_test_original - y_pred_original\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(errores, bins=50)\n",
    "plt.title('Distribución de Errores')\n",
    "plt.xlabel('Error de Predicción')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "mejor_modelo.save('mejor_modelov2.h5')\n",
    "import joblib\n",
    "joblib.dump(preprocessor, 'preprocessor.joblib')\n",
    "\n",
    "print(\"\\n=== Modelo y preprocesador guardados ===\")\n",
    "print(\"Modelo guardado como 'mejor_modelo.h5'\")\n",
    "print(\"Preprocesador guardado como 'preprocessor.joblib'\")\n",
    "\n",
    "# Análisis por ciudad\n",
    "print(\"\\n=== Análisis por ciudad ===\")\n",
    "# Combinar predicciones con datos originales para análisis\n",
    "test_results = pd.DataFrame({\n",
    "    'Ciudad': X_test['City'].values,\n",
    "    'Precio_Real': y_test_original.values,\n",
    "    'Precio_Predicho': y_pred_original,\n",
    "    'Error': y_test_original.values - y_pred_original,\n",
    "    'Error_Porcentual': ((y_test_original.values - y_pred_original) / y_test_original.values) * 100\n",
    "})\n",
    "\n",
    "# Análisis por ciudad\n",
    "city_analysis = test_results.groupby('Ciudad').agg({\n",
    "    'Precio_Real': 'mean',\n",
    "    'Precio_Predicho': 'mean',\n",
    "    'Error': ['mean', 'std'],\n",
    "    'Error_Porcentual': ['mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "display(city_analysis)\n",
    "\n",
    "# Visualizar rendimiento por ciudad\n",
    "plt.figure(figsize=(12, 6))\n",
    "city_mape = test_results.groupby('Ciudad')['Error_Porcentual'].apply(lambda x: np.mean(np.abs(x)))\n",
    "city_mape.sort_values().plot(kind='bar')\n",
    "plt.title('Error Porcentual Absoluto Medio (MAPE) por Ciudad')\n",
    "plt.xlabel('Ciudad')\n",
    "plt.ylabel('MAPE (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Guardar resumen de resultados\n",
    "df_resultados.to_csv('resultados_experimentos.csv', index=False)\n",
    "print(\"Resumen de resultados guardado como 'resultados_experimentos.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
